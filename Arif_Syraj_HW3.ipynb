{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# For interactivity\n",
    "# %matplotlib widget\n",
    "\n",
    "# All imports\n",
    "from random import choice\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set number of decimal places to show\n",
    "np.set_printoptions(formatter={'float': '{:.4f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHelpful functions: np.concatenate\\nCan choose axis to concat along\\n'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Live updating:\n",
    "use display.clear_output(wait=True)\n",
    "Make an array for figures\n",
    "Save figure by doing plt.plot, then append, then plt.close\n",
    "Then to live update, display each figure using clear_output and then display.display()\n",
    "use plt.clf() to clear figure\n",
    "Maybe can stream instead of saving all figs?\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Helpful functions: np.concatenate\n",
    "Can choose axis to concat along\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: float) -> float:\n",
    "    \"\"\"Returns the sigmoid\n",
    "\n",
    "    Args:\n",
    "        x (float): the input\n",
    "\n",
    "    Returns:\n",
    "        float: the sigmoid of the input\n",
    "    \"\"\"\n",
    "    return 1./(1. + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x: float) -> float:\n",
    "    \"\"\"Return the derivative of the sigmoid function\n",
    "\n",
    "    Args:\n",
    "        x (float): the input\n",
    "\n",
    "    Returns:\n",
    "        float: the derivative of the sigmoid of the input\n",
    "    \"\"\"\n",
    "    return x*(1.-x)\n",
    "\n",
    "def tanh(x: float) -> float:\n",
    "    \"\"\"Returns the hyperbolic tangent\n",
    "\n",
    "    Args:\n",
    "        x (float): input\n",
    "\n",
    "    Returns:\n",
    "        float: hyperbolic tangent of x\n",
    "    \"\"\"\n",
    "    return ((np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x)))\n",
    "\n",
    "def tanh_prime(x: float) -> float:\n",
    "    \"\"\"Returns the derivative of the hyperbolic tangent\n",
    "\n",
    "    Args:\n",
    "        x (float): input\n",
    "\n",
    "    Returns:\n",
    "        float: derivative of the hyperbolic tangent of x\n",
    "    \"\"\"\n",
    "    g = tanh(x)\n",
    "    return (1. - g**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers: list, activation: str='sigmoid', verbose=False):\n",
    "        \"\"\"Cosntructor for neural network\n",
    "\n",
    "        Args:\n",
    "            layers (list): number of inputs, hidden layers, and outputs as a list\n",
    "            activation (str, optional): activation function to use. Only sigmoid and tanh are available. Defaults to 'signmoid'.\n",
    "        \"\"\"\n",
    "        self.num_layers = len(layers)\n",
    "        self.num_inputs = layers[0]\n",
    "        self.num_outputs = layers[-1]\n",
    "        self.num_hidden_layers = len(layers) - 1\n",
    "        self.layers = layers\n",
    "        if verbose:\n",
    "            print(f\"num_inputs = {self.num_inputs}\\nnum_outputs = {self.num_outputs}\\nlayers = {self.layers}\")\n",
    "        \n",
    "        # Create random weights\n",
    "        self.weights = []\n",
    "        for i in range(self.num_layers - 1):\n",
    "            # Add bias weight for each neuron if not last layer\n",
    "            if i == self.num_layers - 1:\n",
    "                curr_weights = np.random.rand(layers[i], layers[i+1])\n",
    "            else:\n",
    "                curr_weights = np.random.rand(layers[i] + 1, layers[i+1])\n",
    "            self.weights.append(curr_weights)\n",
    "\n",
    "        if verbose:\n",
    "            for layer, weights in enumerate(self.weights):\n",
    "                print(f\"weights[{layer}]:\\n{weights}\")\n",
    "                print(f\"weights[{layer}].shape: {weights.shape}\")\n",
    "                print()\n",
    "        \n",
    "        if activation == \"sigmoid\":\n",
    "            self.activation = sigmoid\n",
    "            self.activation_derivative = sigmoid_prime\n",
    "        elif activation == \"tanh\":\n",
    "            self.activation = tanh\n",
    "            self.activation_derivative = tanh_prime\n",
    "        else:\n",
    "            raise Exception(\"Activation function must be 'sigmoid' or 'tanh'\")\n",
    "\n",
    "        # Array to store post-activations for each layer; for n layers, there are n - 1 activations, as layer n - 1, the output layer,\n",
    "        # does not have an activation\n",
    "        self.activations = [np.array([]) for _ in range(self.num_layers - 1)]\n",
    "        # Array to store inputs for each layer. Last layer's 'input' is the activation of the last hidden layer\n",
    "        self.inputs = [np.array([]) for _ in range(self.num_layers)]\n",
    "        self.errors = []\n",
    "        self.errors_squared = []\n",
    "        self.is_trained = False\n",
    "\n",
    "    def forward_prop(self, X: np.ndarray, verbose: bool=False):\n",
    "        \"\"\"runs forward propagation through the NN for a given input.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): the training sample\n",
    "            verbose (bool, optional): Flag to print out statement. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        # Add to inputs array\n",
    "        X = np.atleast_2d(X) # Convert to 2D matrix\n",
    "        # Add bias to first input\n",
    "        X = np.concatenate((X, np.ones((1, 1))), axis=1)\n",
    "        self.inputs[0] = X\n",
    "        if verbose:\n",
    "            print(\"inputs:\", self.inputs[0])\n",
    "        \n",
    "        for layer, weight in enumerate(self.weights):\n",
    "            curr_layer_input = self.inputs[layer]\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"layer {layer}, curr_input.shape: {curr_layer_input.shape}\")\n",
    "                print(f\"Layer {layer}, curr_input: {curr_layer_input}\")\n",
    "                print(self.inputs)\n",
    "            \n",
    "            dot_product = np.dot(curr_layer_input, weight)\n",
    "            z_curr_layer = np.atleast_2d(self.activation(dot_product)) # Apply activation function\n",
    "            \n",
    "            # Save activation of curr layer\n",
    "            self.activations[layer] = z_curr_layer\n",
    "            # Save input to next layer after adding bias if next layer is not output layer\n",
    "            if not layer == self.num_layers - 2:\n",
    "                next_layer_input = np.concatenate((z_curr_layer, np.ones((1, 1))), axis=1)\n",
    "                self.inputs[layer + 1] = next_layer_input\n",
    "            else:\n",
    "                self.inputs[layer + 1] = z_curr_layer\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"activations:\")\n",
    "            print(self.activations)\n",
    "            print(\"inputs:\")\n",
    "            print(self.inputs)\n",
    "        # self.activations has length n - 1. activations[0] is the output of layer 0, so activations[n-2] is the \n",
    "        # output of the last hidden layer, aka the output of the NN. Layer n - 1 is the output layer and has no \n",
    "        # activations\n",
    "        return self.activations[-1]\n",
    "    \n",
    "    def __backprop(self, alpha: float = 0.02, verbose=False):\n",
    "        \"\"\"back propagation method that updates weights for an NN by sampling from \n",
    "\n",
    "        Args:\n",
    "            alpha (float, optional): learning rate. Defaults to 0.02.\n",
    "            verbose (bool, optional): whether to print out statements. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise Exception(\"backprop can only be called through the fit() method\")\n",
    "        # Keep track of deltas, weight updates, and errors at each pass\n",
    "        deltas = [np.array([]) for _ in range(self.num_hidden_layers)]\n",
    "        weight_updates = [np.array([]) for _ in range(self.num_hidden_layers)]\n",
    "        sample = np.random.choice(len(self.X))\n",
    "        curr_sample = self.X[sample]\n",
    "        curr_target = self.target[sample]\n",
    "        if isinstance(curr_sample, np.ndarray) and not len(curr_sample) == self.num_inputs:\n",
    "                raise Exception(\"Number of inputs must match those specified in architecture\")\n",
    "        if isinstance(curr_target, np.ndarray) and not len(curr_target) == self.num_outputs:\n",
    "                raise Exception(\"Number of outputs must match those specified in architecture and be supplied as a list\")\n",
    "\n",
    "        z = self.forward_prop(curr_sample, False)\n",
    "        if verbose:\n",
    "            print(z)\n",
    "        curr_target = np.atleast_2d(curr_target)\n",
    "        error = np.array(curr_target - z)\n",
    "        self.errors.append(error.squeeze())\n",
    "        self.errors_squared.append(error**2)\n",
    "\n",
    "        # output layer delta has diff formula\n",
    "        delta_last = (curr_target - z)*self.activation_derivative(z)\n",
    "        deltas[-1] = delta_last\n",
    "        # self.inputs' last element is the NN output, so one before that is the input for the last hidden layer\n",
    "        delta_weights_output = np.dot(self.inputs[self.num_layers - 2].T, delta_last)*alpha\n",
    "        weight_updates[-1] = delta_weights_output\n",
    "        \n",
    "        self.weights[-1] += delta_weights_output\n",
    "        if verbose:\n",
    "            print(f\"Backprop for layer {self.num_hidden_layers - 1} done\\n\")\n",
    "        # Start backprop-ing starting from layer (n-3), as layer (n-2) weights have been updated.\n",
    "        for curr_layer in range(self.num_layers - 3, -1, -1):\n",
    "            if verbose: \n",
    "                print(f\"backprop from layer {curr_layer + 1} to {curr_layer} out of {self.num_layers} layers\")\n",
    "            \n",
    "            # Drop the bias activation value from the next layer, so do not take input of next layer, \n",
    "            # just activation of current layer. This is z^(i)\n",
    "            next_layer_activations = self.activations[curr_layer]\n",
    "            next_layer_weights = self.weights[curr_layer + 1]\n",
    "            # Drop the weights of the bias neuron from next layer\n",
    "            next_layer_weights = next_layer_weights[:-1, :]\n",
    "            \n",
    "            delta_curr_layer = sigmoid_prime(next_layer_activations)*np.dot(deltas[curr_layer + 1], next_layer_weights.T)\n",
    "            deltas[curr_layer] = delta_curr_layer\n",
    "            curr_inputs = self.inputs[curr_layer]\n",
    "            weight_update_curr_layer = alpha*np.dot(curr_inputs.T, delta_curr_layer)\n",
    "            weight_updates[curr_layer] = weight_update_curr_layer\n",
    "            self.weights[curr_layer] += weight_update_curr_layer\n",
    "            if verbose: \n",
    "                print(f\"Backprop for layer {curr_layer} done\")\n",
    "                # print(f\"Weight updates:\\n{weight_update_curr_layer}\")\n",
    "                print()\n",
    "        \n",
    "        return 0\n",
    "\n",
    "    def fit(self, X:np.array, y:np.array, learning_rate:float=0.2, steps:float=10**5, tol:float = 10**-2, verbose:bool = False):\n",
    "        \"\"\"Method to train a neural network\n",
    "\n",
    "        Args:\n",
    "            X (np.array): Training data\n",
    "            y (np.array): Target data\n",
    "            learning_rate (float, optional): learning rate of the NN. Defaults to 0.2.\n",
    "            steps (float, optional): no. steps to train for. Defaults to 10**5.\n",
    "            tol (float, optional): error value below which training exits. Defaults to 10**-2.\n",
    "            verbose (bool, optional): Flag to print out statements. Defaults to False.\n",
    "        \"\"\"\n",
    "        if not isinstance(X, np.ndarray) or not X.any():\n",
    "            raise Exception(\"Input must be given and be a numpy array\")\n",
    "        if not isinstance(y, np.ndarray) or not y.any():\n",
    "            raise Exception(\"Target outputs must be supplied as a numpy array\")\n",
    "        if not len(X) == len(y):\n",
    "            raise Exception(f\"Training data length must match target data length {X.shape} != {y.shape}\")\n",
    "        self.X = X\n",
    "        self.target = y\n",
    "        self.is_trained = True\n",
    "        old_weights = self.weights\n",
    "\n",
    "        for iter in range(0, steps + 1):\n",
    "            self.__backprop(learning_rate, verbose)\n",
    "\n",
    "            if (iter) % (int(steps*0.1)) == 0:\n",
    "                print(f\"Step: {iter}\")\n",
    "                print(\"Training Results(data, prediction, expected):\")\n",
    "                self.predict_many(self.X, self.target)\n",
    "                # RMS_error = self.__find_RMS_error_across_all_errors(int(max(0, iter - steps*0.1)))\n",
    "                RMS_error = self.__find_RMS_error()\n",
    "                print(f\"RMS_err: {RMS_error}\\n\")\n",
    "                if RMS_error <= tol:\n",
    "                    print(\"NN training succeded!\")\n",
    "                    return\n",
    "        if RMS_error > tol:\n",
    "            print(\"NN training failed.\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def __find_RMS_error(self):\n",
    "        \"\"\"private method to find RMS error\n",
    "\n",
    "        Args:\n",
    "            X (_type_): _description_\n",
    "            y (_type_): _description_\n",
    "        \"\"\"\n",
    "        rms_errors = []\n",
    "        for i, sample in enumerate(self.X):\n",
    "            output = self.forward_prop(sample)\n",
    "            error = self.target[i] - output\n",
    "            rms_errors.append(error)\n",
    "        return np.sqrt(np.mean(np.array(rms_errors)**2))\n",
    "    \n",
    "    def __find_RMS_error_across_all_errors(self, start_idx):\n",
    "        \"\"\"private method to find RMS error\n",
    "\n",
    "        Args:\n",
    "            X (_type_): _description_\n",
    "            y (_type_): _description_\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise Exception(\"RMS error can only be calculated through the fit() method\")\n",
    "        return np.sqrt(np.mean(np.array(self.errors[start_idx:])**2))\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Uses the input to predict an output according to NN weights.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): input\n",
    "        \"\"\"\n",
    "        return self.forward_prop(x, False)\n",
    "    \n",
    "    def predict_many(self, X, y):\n",
    "        for i, sample in enumerate(X):\n",
    "            print(f\"{sample}, {self.predict(sample)[0][0]}, {y[i]}\")\n",
    "\n",
    "    def visual_NN_boundaries(self, Nsamp=2000):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            Nsamp (int, optional): _description_. Defaults to 2000.\n",
    "        \"\"\"\n",
    "        # predictions = []\n",
    "        # for iter in range(Nsamp)\n",
    "        return\n",
    "\n",
    "    def __smooth_errors(self, window_size=10):\n",
    "        window = np.ones(window_size) / window_size\n",
    "        return np.convolve(np.array(self.errors_squared).flatten(), window, mode='valid')\n",
    "\n",
    "    def get_smoothed_errors_squared(self, window_size=10):\n",
    "        smoothed_errors = self.__smooth_errors(min(window_size, len(self.errors_squared)))\n",
    "        return np.array(self.errors_squared), smoothed_errors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "Training Results(data, prediction, expected):\n",
      "[0 0], 0.7004771087529824, 0\n",
      "[0 1], 0.7234423888500304, 1\n",
      "[1 0], 0.7250938487843417, 1\n",
      "[1 1], 0.7426487784335495, 0\n",
      "RMS_err: 0.5464093914495259\n",
      "\n",
      "Step: 20000\n",
      "Training Results(data, prediction, expected):\n",
      "[0 0], 0.017201999904862667, 0\n",
      "[0 1], 0.984969522381587, 1\n",
      "[1 0], 0.984688815930047, 1\n",
      "[1 1], 0.015318965439949142, 0\n",
      "RMS_err: 0.01573949743431195\n",
      "\n",
      "Step: 40000\n",
      "Training Results(data, prediction, expected):\n",
      "[0 0], 0.011759122966994707, 0\n",
      "[0 1], 0.9900119822982901, 1\n",
      "[1 0], 0.9897776647148486, 1\n",
      "[1 1], 0.010404006860499294, 0\n",
      "RMS_err: 0.010615754424414914\n",
      "\n",
      "Step: 60000\n",
      "Training Results(data, prediction, expected):\n",
      "[0 0], 0.009480998499633258, 0\n",
      "[0 1], 0.991871863959745, 1\n",
      "[1 0], 0.9917855866302359, 1\n",
      "[1 1], 0.008178209222437784, 0\n",
      "RMS_err: 0.008519325400732533\n",
      "\n",
      "NN training succeded!\n"
     ]
    }
   ],
   "source": [
    "XOR = NeuralNetwork([2, 2, 1], activation='sigmoid', verbose=False)\n",
    "\n",
    "X = np.array([[0, 0],\n",
    "            [0, 1],\n",
    "            [1, 0],\n",
    "            [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "steps = 2*10**5\n",
    "XOR.fit(X, \n",
    "         y,\n",
    "         steps=steps,\n",
    "         learning_rate=1.2,\n",
    "         verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show smoothed loss function vs training steps\n",
    "window_size = 100\n",
    "errors_squared, smoothed_errors = XOR.get_smoothed_errors_squared(window_size)\n",
    "errors_squared = errors_squared.flatten()\n",
    "num_errors = len(errors_squared)\n",
    "x_errors = np.linspace(0, num_errors, num_errors)\n",
    "x_smoothed = np.linspace(window_size, num_errors, len(smoothed_errors))\n",
    "origin = np.zeros(num_errors)\n",
    "plt.plot(x_errors, origin, '--', label='origin', color='orange')\n",
    "# plt.plot(x_errors, errors_squared, label=\"errors_squared raw\", color='blue')\n",
    "plt.plot(x_smoothed, smoothed_errors, label=\"errors_squared smoothed\", color='cyan')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "Training Results(data, prediction, expected):\n",
      "[0.0000 0.0000], 0.7718880350987595, 0\n",
      "[0.0000 1.0000], 0.7735992396244773, 0\n",
      "[0.5000 1.0000], 0.7766090654027695, 0\n",
      "[0.0000 0.5000], 0.7727750900418506, 1\n",
      "[1.0000 0.0000], 0.7777931497836112, 1\n",
      "[1.0000 1.0000], 0.779215044218428, 1\n",
      "RMS_err: 0.5696697293798113\n",
      "\n",
      "Step: 20000\n",
      "Training Results(data, prediction, expected):\n",
      "[0.0000 0.0000], 0.7410023127135162, 0\n",
      "[0.0000 1.0000], 0.05245265250074869, 0\n",
      "[0.5000 1.0000], 0.036298100400263496, 0\n",
      "[0.0000 0.5000], 0.740792306690468, 1\n",
      "[1.0000 0.0000], 0.9996127138600228, 1\n",
      "[1.0000 1.0000], 0.9958185887244164, 1\n",
      "RMS_err: 0.32154821379161524\n",
      "\n",
      "Step: 40000\n",
      "Training Results(data, prediction, expected):\n",
      "[0.0000 0.0000], 0.029999259765185804, 0\n",
      "[0.0000 1.0000], 0.01929793926785371, 0\n",
      "[0.5000 1.0000], 0.02357227021083419, 0\n",
      "[0.0000 0.5000], 0.9682662015798937, 1\n",
      "[1.0000 0.0000], 0.9930568476442785, 1\n",
      "[1.0000 1.0000], 0.9841517048017284, 1\n",
      "RMS_err: 0.022856172604760788\n",
      "\n",
      "Step: 60000\n",
      "Training Results(data, prediction, expected):\n",
      "[0.0000 0.0000], 0.019940842989039635, 0\n",
      "[0.0000 1.0000], 0.012542431806690943, 0\n",
      "[0.5000 1.0000], 0.015137752751100736, 0\n",
      "[0.0000 0.5000], 0.9800796788238657, 1\n",
      "[1.0000 0.0000], 0.9950286037532752, 1\n",
      "[1.0000 1.0000], 0.9894937623566681, 1\n",
      "RMS_err: 0.014809998755980932\n",
      "\n",
      "Step: 80000\n",
      "Training Results(data, prediction, expected):\n",
      "[0.0000 0.0000], 0.015419932786158583, 0\n",
      "[0.0000 1.0000], 0.009924134053873553, 0\n",
      "[0.5000 1.0000], 0.011962088675593998, 0\n",
      "[0.0000 0.5000], 0.9838434552586354, 1\n",
      "[1.0000 0.0000], 0.9957859145087231, 1\n",
      "[1.0000 1.0000], 0.9915484396517136, 1\n",
      "RMS_err: 0.011758512208608069\n",
      "\n",
      "Step: 100000\n",
      "Training Results(data, prediction, expected):\n",
      "[0.0000 0.0000], 0.013205291054987573, 0\n",
      "[0.0000 1.0000], 0.008533243032921986, 0\n",
      "[0.5000 1.0000], 0.010248833730439572, 0\n",
      "[0.0000 0.5000], 0.9863755834280923, 1\n",
      "[1.0000 0.0000], 0.9963082440044816, 1\n",
      "[1.0000 1.0000], 0.9927835590783904, 1\n",
      "RMS_err: 0.010029667244151582\n",
      "\n",
      "Step: 120000\n",
      "Training Results(data, prediction, expected):\n",
      "[0.0000 0.0000], 0.011790642091801473, 0\n",
      "[0.0000 1.0000], 0.00752635682652855, 0\n",
      "[0.5000 1.0000], 0.009024358198995285, 0\n",
      "[0.0000 0.5000], 0.9879386076293442, 1\n",
      "[1.0000 0.0000], 0.9966493719822531, 1\n",
      "[1.0000 1.0000], 0.993579112758874, 1\n",
      "RMS_err: 0.008897902523595536\n",
      "\n",
      "NN training succeded!\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([2, 2, 2, 1], activation='sigmoid', verbose=False)\n",
    "\n",
    "X = np.array([[0, 0],\n",
    "            [0, 1],\n",
    "            [0.5, 1],\n",
    "            [0, 0.5],\n",
    "            [1, 0],\n",
    "            [1, 1]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "nn.fit(X, \n",
    "         y,\n",
    "         steps=2*10**5,\n",
    "         learning_rate=1.2,\n",
    "         verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 1)\n",
      "(3,)\n",
      "[[[1]]\n",
      "\n",
      " [[1]]\n",
      "\n",
      " [[1]]]\n",
      "(3,)\n",
      "[1 1 1]\n",
      "[1 1 1]\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[[1]], [[1]], [[1]]])\n",
    "print(t.shape)\n",
    "print(np.ravel(t).shape)\n",
    "print(t)\n",
    "print(t.flatten().shape)\n",
    "print(t.flatten())\n",
    "print(np.ravel(t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
