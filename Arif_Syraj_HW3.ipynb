{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# For interactivity\n",
    "# %matplotlib widget\n",
    "\n",
    "# All imports\n",
    "from random import choice\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set number of decimal places to show\n",
    "np.set_printoptions(formatter={'float': '{:.4f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHelpful functions: np.concatenate\\nCan choose axis to concat along\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Live updating:\n",
    "use display.clear_output(wait=True)\n",
    "Make an array for figures\n",
    "Save figure by doing plt.plot, then append, then plt.close\n",
    "Then to live update, display each figure using clear_output and then display.display()\n",
    "use plt.clf() to clear figure\n",
    "Maybe can stream instead of saving all figs?\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Helpful functions: np.concatenate\n",
    "Can choose axis to concat along\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: float) -> float:\n",
    "    \"\"\"Returns the sigmoid\n",
    "\n",
    "    Args:\n",
    "        x (float): the input\n",
    "\n",
    "    Returns:\n",
    "        float: the sigmoid of the input\n",
    "    \"\"\"\n",
    "    return 1./(1. + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x: float) -> float:\n",
    "    \"\"\"Return the derivative of the sigmoid function\n",
    "\n",
    "    Args:\n",
    "        x (float): the input\n",
    "\n",
    "    Returns:\n",
    "        float: the derivative of the sigmoid of the input\n",
    "    \"\"\"\n",
    "    return x*(1.-x)\n",
    "\n",
    "def tanh(x: float) -> float:\n",
    "    \"\"\"Returns the hyperbolic tangent\n",
    "\n",
    "    Args:\n",
    "        x (float): input\n",
    "\n",
    "    Returns:\n",
    "        float: hyperbolic tangent of x\n",
    "    \"\"\"\n",
    "    return ((np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x)))\n",
    "\n",
    "def tanh_prime(x: float) -> float:\n",
    "    \"\"\"Returns the derivative of the hyperbolic tangent\n",
    "\n",
    "    Args:\n",
    "        x (float): input\n",
    "\n",
    "    Returns:\n",
    "        float: derivative of the hyperbolic tangent of x\n",
    "    \"\"\"\n",
    "    g = tanh(x)\n",
    "    return (1. - g**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers: list, activation: str='sigmoid', verbose=False):\n",
    "        \"\"\"Cosntructor for neural network\n",
    "\n",
    "        Args:\n",
    "            layers (list): number of inputs, hidden layers, and outputs as a list\n",
    "            activation (str, optional): activation function to use. Only sigmoid and tanh are available. Defaults to 'signmoid'.\n",
    "        \"\"\"\n",
    "        self.num_layers = len(layers)\n",
    "        self.num_inputs = layers[0]\n",
    "        self.num_outputs = layers[-1]\n",
    "        self.num_hidden_layers = len(layers) - 2\n",
    "        self.layers = layers\n",
    "        if verbose:\n",
    "            print(f\"num_inputs = {self.num_inputs}\\nnum_outputs = {self.num_outputs}\\nlayers = {self.layers}\")\n",
    "        \n",
    "        # Create random weights\n",
    "        self.weights = []\n",
    "        for i in range(self.num_layers - 1):\n",
    "            # Add bias weight for each neuron if not last layer\n",
    "            if i == self.num_layers - 1:\n",
    "                curr_weights = np.random.rand(layers[i], layers[i+1])\n",
    "            else:\n",
    "                curr_weights = np.random.rand(layers[i] + 1, layers[i+1])\n",
    "            self.weights.append(curr_weights)\n",
    "\n",
    "        if verbose:\n",
    "            for layer, weights in enumerate(self.weights):\n",
    "                print(f\"weights[{layer}]:\\n{weights}\")\n",
    "                print(f\"weights[{layer}].shape: {weights.shape}\")\n",
    "                print()\n",
    "        \n",
    "        if activation == \"sigmoid\":\n",
    "            self.activation = sigmoid\n",
    "        elif activation == \"tanh\":\n",
    "            self.activation = tanh\n",
    "        else:\n",
    "            raise Exception(\"Activation function must be 'sigmoid' or 'tanh'\")\n",
    "\n",
    "        self.activations = []\n",
    "        self.inputs = []\n",
    "        self.errors = []\n",
    "\n",
    "    def forward_prop(self, X, verbose=False):\n",
    "        # Add bias\n",
    "        X = np.concatenate((X, np.ones(1))) # Add bias\n",
    "        X = np.atleast_2d(X) # Convert to 2D matrix\n",
    "        # post-activations array\n",
    "        a_arr = [X]\n",
    "        for layer, weight in enumerate(self.weights):\n",
    "            # 1x4\n",
    "            curr_layer_input = a_arr[layer]\n",
    "            # print(f\"layer {layer}, curr_input.shape: {curr_layer_input.shape}\")\n",
    "            if not (layer < self.num_layers - 2): # Add bias upto layer preceding output layer\n",
    "                # curr_layer_input = np.concatenate((curr_layer_input, np.ones((1, 1))), axis=1)\n",
    "                pass\n",
    "            if verbose:\n",
    "                print(f\"Layer {layer}\")\n",
    "                print(curr_layer_input.shape)\n",
    "                print(curr_layer_input[0].shape)\n",
    "            # print(f\"layer {layer}, curr_input.shape: {curr_layer_input.shape}\")\n",
    "            # weight = 4x2\n",
    "            dot_product = np.dot(curr_layer_input, weight)\n",
    "            z_layer = np.atleast_2d(self.activation(dot_product))\n",
    "            if layer < self.num_layers - 3:\n",
    "                # print(f\"layer {layer}, z_layer.shape: {z_layer.shape}\")\n",
    "                z_layer = np.concatenate((z_layer, np.ones((1, 1))), axis=1)\n",
    "                # print(f\"layer {layer}, z_layer.shape: {z_layer.shape}\")\n",
    "            a_arr.append(z_layer)\n",
    "        if verbose: \n",
    "            print(a_arr)\n",
    "        self.activations = a_arr # Save activations at each layer\n",
    "        return a_arr[-1]\n",
    "    \n",
    "    def backprop(self, X: list, y: list, alpha: float = 0.02, verbose=False):\n",
    "        # Keep track of deltas, weight updates, and errors at each pass\n",
    "        deltas = []\n",
    "        weight_updates = []\n",
    "        \n",
    "        z = self.forward_prop(X, False)\n",
    "        if verbose:\n",
    "            print(z)\n",
    "        y = np.atleast_2d(y)\n",
    "        error = y - z\n",
    "        self.errors.append(error)\n",
    "\n",
    "        # output layer delta has diff formula. np's '*' operator is element-wise multiplication\n",
    "        delta_output = (y-z)*sigmoid_prime(z) # (n, 1) matrix\n",
    "        deltas.append(delta_output)\n",
    "\n",
    "        delta_weights_output = np.dot(self.activations[-2].T, delta_output)*alpha\n",
    "        weight_updates.insert(0, delta_weights_output)\n",
    "        # print(f\"updates: {weight_updates}\")\n",
    "        self.weights[-1] += delta_weights_output\n",
    "        # Start backprop-ing starting from layer (n-3), as layer (n-1) is the output layer, \n",
    "        # and layer (n-2) is the last hidden layer, and n-2 weights have been updated\n",
    "        for i in range(self.num_layers - 3, -1, -1):\n",
    "            print(f\"backprop from layer {i + 1} to {i} out of {self.num_layers - 1} layers\")\n",
    "            next_layer_activations = self.activations[i+1]\n",
    "            print(f\"In layer {i};\\nlayer {i+1} activations = {next_layer_activations}\\nlayer {i+1} activations.shape: = {next_layer_activations.shape}\")\n",
    "            delta_i = sigmoid_prime(next_layer_activations)*np.dot(deltas[0], self.weights[i+1].T)\n",
    "            deltas.insert(0, delta_i)\n",
    "            curr_inputs = self.activations[i]\n",
    "            print(f\"curr inputs shape: {curr_inputs.shape}\")\n",
    "            delta_weights_i = alpha*np.dot(delta_i, curr_inputs.T)\n",
    "            weight_updates.insert(0, delta_weights_i)\n",
    "            self.weights[i] += delta_weights_i\n",
    "            print(f\"Backprop for layer {i} done\")\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def fit(self, X:list, y:list, learning_rate:float=0.2, steps:float=10**5, tolerance:float = 10**-2, verbose:bool = False):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            X (list): _description_\n",
    "            y (list): _description_\n",
    "            learning_rate (float, optional): _description_. Defaults to 0.2.\n",
    "            steps (float, optional): _description_. Defaults to 10**5.\n",
    "            tolerance (float, optional): _description_. Defaults to 10**-2.\n",
    "            verbose (bool, optional): _description_. Defaults to False.\n",
    "        \"\"\"\n",
    "        if not X or (len(X) != self.num_inputs):\n",
    "            raise Exception(\"Number of inputs must match those specified in architecture\")\n",
    "        if not y or not isinstance(y, list) or (self.num_outputs > 1 and len(y) != self.num_outputs):\n",
    "            raise Exception(\"Number of outputs must match those specified in architecture and be supplied as a list\")\n",
    "        self.backprop(X, y, learning_rate)\n",
    "        return\n",
    "\n",
    "    def find_RMS_error(self, X, y):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            X (_type_): _description_\n",
    "            y (_type_): _description_\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            x (_type_): _description_\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    def visual_NN_boundaries(self, Nsamp=2000):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            Nsamp (int, optional): _description_. Defaults to 2000.\n",
    "        \"\"\"\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backprop from layer 2 to 1 out of 3 layers\n",
      "In layer 1;\n",
      "layer 2 activations = [[0.8634 0.8208 0.7292]]\n",
      "layer 2 activations.shape: = (1, 3)\n",
      "curr inputs shape: (1, 3)\n",
      "Backprop for layer 1 done\n",
      "backprop from layer 1 to 0 out of 3 layers\n",
      "In layer 0;\n",
      "layer 1 activations = [[0.7167 0.7242 1.0000]]\n",
      "layer 1 activations.shape: = (1, 3)\n",
      "curr inputs shape: (1, 4)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,3) and (4,1) not aligned: 3 (dim 1) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test \u001b[38;5;241m=\u001b[39m NeuralNetwork([\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# test = NeuralNetwork([5, 2, 2, 3, 1])\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[76], line 126\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[0;34m(self, X, y, learning_rate, steps, tolerance, verbose)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of outputs must match those specified in architecture and be supplied as a list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[76], line 103\u001b[0m, in \u001b[0;36mNeuralNetwork.backprop\u001b[0;34m(self, X, y, alpha, verbose)\u001b[0m\n\u001b[1;32m    101\u001b[0m curr_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations[i]\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurr inputs shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurr_inputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m delta_weights_i \u001b[38;5;241m=\u001b[39m alpha\u001b[38;5;241m*\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m weight_updates\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, delta_weights_i)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m delta_weights_i\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,3) and (4,1) not aligned: 3 (dim 1) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "test = NeuralNetwork([3, 2, 3, 2], verbose=True)\n",
    "# test.fit([0.3, 0.4, 0.5], [0.5, 1.0])\n",
    "# test = NeuralNetwork([5, 2, 2, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updates: [array([[-0.0065, 0.0057],\n",
      "       [-0.0073, 0.0064],\n",
      "       [-0.0079, 0.0069]])]\n",
      "layer 1\n",
      "(1, 2)\n",
      "(1, 3)\n",
      "layer 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,2) (1,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[222], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[217], line 117\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[0;34m(self, X, y, learning_rate, steps, tolerance, verbose)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of outputs must match those specified in architecture and be supplied as a list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[217], line 87\u001b[0m, in \u001b[0;36mNeuralNetwork.backprop\u001b[0;34m(self, X, y, alpha, verbose)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 87\u001b[0m     delta_i \u001b[38;5;241m=\u001b[39m \u001b[43msigmoid_prime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeltas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     deltas\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, delta_i)\n\u001b[1;32m     89\u001b[0m     curr_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations[i]\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,2) (1,3) "
     ]
    }
   ],
   "source": [
    "test.fit([0.3, 0.4, 0.5], [0.5, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8326]\n",
      " [0.3836]\n",
      " [0.1912]]\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "[[8.3255 83.2553]\n",
      " [3.8357 38.3566]\n",
      " [1.9123 19.1229]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[8.3255, 84.2553],\n",
       "       [5.8357, 41.3566],\n",
       "       [5.9123, 24.1229]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# targ = np.array(([10], [0]))\n",
    "# out = np.array(([10], [0]))\n",
    "# out*(1-out)\n",
    "\n",
    "w_test = np.arange(6).reshape((3, 2))\n",
    "a_test = np.random.rand(3,1)\n",
    "deltas = (10*np.ones(2)).reshape((2, 1))\n",
    "deltas[1][0] *= 10\n",
    "# deltas[2][0] *= 100\n",
    "updates = np.dot(a_test, deltas.T)\n",
    "print(a_test)\n",
    "print(w_test)\n",
    "print(updates)\n",
    "updates + w_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
