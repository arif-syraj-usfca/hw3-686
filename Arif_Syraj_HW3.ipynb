{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# For interactivity\n",
    "# %matplotlib widget\n",
    "\n",
    "# All imports\n",
    "from random import choice\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set number of decimal places to show\n",
    "np.set_printoptions(formatter={'float': '{:.4f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHelpful functions: np.concatenate\\nCan choose axis to concat along\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Live updating:\n",
    "use display.clear_output(wait=True)\n",
    "Make an array for figures\n",
    "Save figure by doing plt.plot, then append, then plt.close\n",
    "Then to live update, display each figure using clear_output and then display.display()\n",
    "use plt.clf() to clear figure\n",
    "Maybe can stream instead of saving all figs?\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Helpful functions: np.concatenate\n",
    "Can choose axis to concat along\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: float) -> float:\n",
    "    \"\"\"Returns the sigmoid\n",
    "\n",
    "    Args:\n",
    "        x (float): the input\n",
    "\n",
    "    Returns:\n",
    "        float: the sigmoid of the input\n",
    "    \"\"\"\n",
    "    return 1./(1. + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x: float) -> float:\n",
    "    \"\"\"Return the derivative of the sigmoid function\n",
    "\n",
    "    Args:\n",
    "        x (float): the input\n",
    "\n",
    "    Returns:\n",
    "        float: the derivative of the sigmoid of the input\n",
    "    \"\"\"\n",
    "    return x*(1.-x)\n",
    "\n",
    "def tanh(x: float) -> float:\n",
    "    \"\"\"Returns the hyperbolic tangent\n",
    "\n",
    "    Args:\n",
    "        x (float): input\n",
    "\n",
    "    Returns:\n",
    "        float: hyperbolic tangent of x\n",
    "    \"\"\"\n",
    "    return ((np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x)))\n",
    "\n",
    "def tanh_prime(x: float) -> float:\n",
    "    \"\"\"Returns the derivative of the hyperbolic tangent\n",
    "\n",
    "    Args:\n",
    "        x (float): input\n",
    "\n",
    "    Returns:\n",
    "        float: derivative of the hyperbolic tangent of x\n",
    "    \"\"\"\n",
    "    g = tanh(x)\n",
    "    return (1. - g**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers: list, activation: str='sigmoid', verbose=False):\n",
    "        \"\"\"Cosntructor for neural network\n",
    "\n",
    "        Args:\n",
    "            layers (list): number of inputs, hidden layers, and outputs as a list\n",
    "            activation (str, optional): activation function to use. Only sigmoid and tanh are available. Defaults to 'signmoid'.\n",
    "        \"\"\"\n",
    "        self.num_layers = len(layers)\n",
    "        self.num_inputs = layers[0]\n",
    "        self.num_outputs = layers[-1]\n",
    "        self.num_hidden_layers = len(layers) - 1\n",
    "        self.layers = layers\n",
    "        if verbose:\n",
    "            print(f\"num_inputs = {self.num_inputs}\\nnum_outputs = {self.num_outputs}\\nlayers = {self.layers}\")\n",
    "        \n",
    "        # Create random weights\n",
    "        self.weights = []\n",
    "        for i in range(self.num_layers - 1):\n",
    "            # Add bias weight for each neuron if not last layer\n",
    "            if i == self.num_layers - 1:\n",
    "                curr_weights = np.random.rand(layers[i], layers[i+1])\n",
    "            else:\n",
    "                curr_weights = np.random.rand(layers[i] + 1, layers[i+1])\n",
    "            self.weights.append(curr_weights)\n",
    "\n",
    "        if verbose:\n",
    "            for layer, weights in enumerate(self.weights):\n",
    "                print(f\"weights[{layer}]:\\n{weights}\")\n",
    "                print(f\"weights[{layer}].shape: {weights.shape}\")\n",
    "                print()\n",
    "        \n",
    "        if activation == \"sigmoid\":\n",
    "            self.activation = sigmoid\n",
    "            self.activation_derivative = sigmoid_prime\n",
    "        elif activation == \"tanh\":\n",
    "            self.activation = tanh\n",
    "            self.activation_derivative = tanh_prime\n",
    "        else:\n",
    "            raise Exception(\"Activation function must be 'sigmoid' or 'tanh'\")\n",
    "\n",
    "        # Array to store post-activations for each layer; for n layers, there are n - 1 activations, as layer n - 1, the output layer,\n",
    "        # does not have an activation\n",
    "        self.activations = [np.array([]) for _ in range(self.num_layers - 1)]\n",
    "        # Array to store inputs for each layer. Last layer's 'input' is the activation of the last hidden layer\n",
    "        self.inputs = [np.array([]) for _ in range(self.num_layers)]\n",
    "        self.errors = []\n",
    "        self.is_trained = False\n",
    "\n",
    "    def forward_prop(self, X: np.ndarray, verbose: bool=False):\n",
    "        \"\"\"runs forward propagation through the NN for a given input.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): the training sample\n",
    "            verbose (bool, optional): Flag to print out statement. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        # Add to inputs array\n",
    "        X = np.atleast_2d(X) # Convert to 2D matrix\n",
    "        # Add bias to first input\n",
    "        X = np.concatenate((X, np.ones((1, 1))), axis=1)\n",
    "        self.inputs[0] = X\n",
    "        if verbose:\n",
    "            print(\"inputs:\", self.inputs[0])\n",
    "        \n",
    "        for layer, weight in enumerate(self.weights):\n",
    "            curr_layer_input = self.inputs[layer]\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"layer {layer}, curr_input.shape: {curr_layer_input.shape}\")\n",
    "                print(f\"Layer {layer}, curr_input: {curr_layer_input}\")\n",
    "                print(self.inputs)\n",
    "            \n",
    "            dot_product = np.dot(curr_layer_input, weight)\n",
    "            z_curr_layer = np.atleast_2d(self.activation(dot_product)) # Apply activation function\n",
    "            \n",
    "            # Save activation of curr layer\n",
    "            self.activations[layer] = z_curr_layer\n",
    "            # Save input to next layer after adding bias if next layer is not output layer\n",
    "            if not layer == self.num_layers - 2:\n",
    "                next_layer_input = np.concatenate((z_curr_layer, np.ones((1, 1))), axis=1)\n",
    "                self.inputs[layer + 1] = next_layer_input\n",
    "            else:\n",
    "                self.inputs[layer + 1] = z_curr_layer\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"activations:\")\n",
    "            print(self.activations)\n",
    "            print(\"inputs:\")\n",
    "            print(self.inputs)\n",
    "        # self.activations has length n - 1. activations[0] is the output of layer 0, so activations[n-2] is the \n",
    "        # output of the last hidden layer, aka the output of the NN. Layer n - 1 is the output layer and has no \n",
    "        # activations\n",
    "        return self.activations[-1]\n",
    "    \n",
    "    def __backprop(self, alpha: float = 0.02, verbose=False):\n",
    "        \"\"\"back propagation method that updates weights for an NN by sampling from \n",
    "\n",
    "        Args:\n",
    "            alpha (float, optional): learning rate. Defaults to 0.02.\n",
    "            verbose (bool, optional): whether to print out statements. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise Exception(\"backprop can only be called through the fit() method\")\n",
    "        # Keep track of deltas, weight updates, and errors at each pass\n",
    "        deltas = [np.array([]) for _ in range(self.num_hidden_layers)]\n",
    "        weight_updates = [np.array([]) for _ in range(self.num_hidden_layers)]\n",
    "        sample = np.random.choice(len(self.X))\n",
    "        curr_sample = self.X[sample]\n",
    "        curr_target = self.target[sample]\n",
    "        if isinstance(curr_sample, np.ndarray) and not len(curr_sample) == self.num_inputs:\n",
    "                raise Exception(\"Number of inputs must match those specified in architecture\")\n",
    "        if isinstance(curr_target, np.ndarray) and not len(curr_target) == self.num_outputs:\n",
    "                raise Exception(\"Number of outputs must match those specified in architecture and be supplied as a list\")\n",
    "\n",
    "        z = self.forward_prop(curr_sample, False)\n",
    "        if verbose:\n",
    "            print(z)\n",
    "        curr_target = np.atleast_2d(curr_target)\n",
    "        self.errors.append(np.array([curr_target - z]))\n",
    "\n",
    "        # output layer delta has diff formula\n",
    "        delta_last = (curr_target - z)*self.activation_derivative(z)\n",
    "        deltas[-1] = delta_last\n",
    "        # self.inputs' last element is the NN output, so one before that is the input for the last hidden layer\n",
    "        delta_weights_output = np.dot(self.inputs[self.num_layers - 2].T, delta_last)*alpha\n",
    "        weight_updates[-1] = delta_weights_output\n",
    "        \n",
    "        self.weights[-1] += delta_weights_output\n",
    "        if verbose:\n",
    "            print(f\"Backprop for layer {self.num_hidden_layers - 1} done\\n\")\n",
    "        # Start backprop-ing starting from layer (n-3), as layer (n-2) weights have been updated.\n",
    "        for curr_layer in range(self.num_layers - 3, -1, -1):\n",
    "            if verbose: \n",
    "                print(f\"backprop from layer {curr_layer + 1} to {curr_layer} out of {self.num_layers} layers\")\n",
    "            \n",
    "            # Drop the bias activation value from the next layer, so do not take input of next layer, \n",
    "            # just activation of current layer. This is z^(i)\n",
    "            next_layer_activations = self.activations[curr_layer]\n",
    "            next_layer_weights = self.weights[curr_layer + 1]\n",
    "            # Drop the weights of the bias neuron from next layer\n",
    "            next_layer_weights = next_layer_weights[:-1, :]\n",
    "            \n",
    "            delta_curr_layer = sigmoid_prime(next_layer_activations)*np.dot(deltas[curr_layer + 1], next_layer_weights.T)\n",
    "            deltas[curr_layer] = delta_curr_layer\n",
    "            curr_inputs = self.inputs[curr_layer]\n",
    "            weight_update_curr_layer = alpha*np.dot(curr_inputs.T, delta_curr_layer)\n",
    "            weight_updates[curr_layer] = weight_update_curr_layer\n",
    "            self.weights[curr_layer] += weight_update_curr_layer\n",
    "            if verbose: \n",
    "                print(f\"Backprop for layer {curr_layer} done\")\n",
    "                # print(f\"Weight updates:\\n{weight_update_curr_layer}\")\n",
    "                print()\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def fit(self, X:np.array, y:np.array, learning_rate:float=0.2, steps:float=10**5, tol:float = 10**-2, verbose:bool = False):\n",
    "        \"\"\"Method to train a neural network\n",
    "\n",
    "        Args:\n",
    "            X (np.array): Training data\n",
    "            y (np.array): Target data\n",
    "            learning_rate (float, optional): learning rate of the NN. Defaults to 0.2.\n",
    "            steps (float, optional): no. steps to train for. Defaults to 10**5.\n",
    "            tol (float, optional): error value below which training exits. Defaults to 10**-2.\n",
    "            verbose (bool, optional): Flag to print out statements. Defaults to False.\n",
    "        \"\"\"\n",
    "        if not isinstance(X, np.ndarray) or not X.any():\n",
    "            raise Exception(\"Input must be given and be a numpy array\")\n",
    "        if not isinstance(y, np.ndarray) or not y.any():\n",
    "            raise Exception(\"Target outputs must be supplied as a numpy array\")\n",
    "        if not len(X) == len(y):\n",
    "            raise Exception(f\"Training data length must match target data length {X.shape} != {y.shape}\")\n",
    "        self.X = X\n",
    "        self.target = y\n",
    "        self.is_trained = True\n",
    "        old_weights = self.weights\n",
    "\n",
    "        for iter in range(0, steps + 1):\n",
    "            self.__backprop(learning_rate, verbose)\n",
    "\n",
    "            if (iter) % (int(steps*0.1)) == 0:\n",
    "                print(f\"Step: {iter}\")\n",
    "                print(\"Training Results(data, prediction, expected):\")\n",
    "                self.predict_many(self.X, self.target)\n",
    "                # RMS_error = self.__find_RMS_error_across_all_errors(int(max(0, iter - steps*0.1)))\n",
    "                RMS_error = self.__find_RMS_error()\n",
    "                print(f\"RMS_err: {RMS_error}\\n\")\n",
    "                if RMS_error <= tol:\n",
    "                    print(\"NN training succeded!\")\n",
    "                    return\n",
    "        if RMS_error > tol:\n",
    "            print(\"NN training failed.\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def __find_RMS_error(self):\n",
    "        \"\"\"private method to find RMS error\n",
    "\n",
    "        Args:\n",
    "            X (_type_): _description_\n",
    "            y (_type_): _description_\n",
    "        \"\"\"\n",
    "        rms_errors = []\n",
    "        for i, sample in enumerate(self.X):\n",
    "            output = self.forward_prop(sample)\n",
    "            error = self.target[i] - output\n",
    "            rms_errors.append(error)\n",
    "        return np.sqrt(np.mean(np.array(rms_errors)**2))\n",
    "    \n",
    "    def __find_RMS_error_across_all_errors(self, start_idx):\n",
    "        \"\"\"private method to find RMS error\n",
    "\n",
    "        Args:\n",
    "            X (_type_): _description_\n",
    "            y (_type_): _description_\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise Exception(\"RMS error can only be calculated through the fit() method\")\n",
    "        return np.sqrt(np.mean(np.array(self.errors[start_idx:])**2))\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Uses the input to predict an output according to NN weights.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): input\n",
    "        \"\"\"\n",
    "        if not isinstance(x, np.ndarray) or not x.any():\n",
    "            raise Exception(\"Input must be given and be a numpy array\")\n",
    "        return self.forward_prop(x, False)\n",
    "    \n",
    "    def predict_many(self, X, y):\n",
    "        for i, sample in enumerate(X):\n",
    "            print(f\"{sample}, {self.predict(sample)[0][0]}, {y[i]}\")\n",
    "\n",
    "    def visual_NN_boundaries(self, Nsamp=2000):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            Nsamp (int, optional): _description_. Defaults to 2000.\n",
    "        \"\"\"\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "Training Results(data, prediction, expected):\n",
      "[0 0], 0.7479822041713873, 0\n",
      "[0 1], 0.7764336428663647, 1\n",
      "[1 0], 0.7529265583571857, 1\n",
      "[1 1], 0.7789808047874591, 0\n",
      "RMS_err: 0.5650919557009358\n",
      "\n",
      "Step: 20000\n",
      "Training Results(data, prediction, expected):\n",
      "[0 0], 0.011666324352969052, 0\n",
      "[0 1], 0.9902060345816263, 1\n",
      "[1 0], 0.990087577772097, 1\n",
      "[1 1], 0.010620522535320871, 0\n",
      "RMS_err: 0.01052469115784952\n",
      "\n",
      "Step: 40000\n",
      "Training Results(data, prediction, expected):\n",
      "[0 0], 0.008068756612559954, 0\n",
      "[0 1], 0.9932848756353067, 1\n",
      "[1 0], 0.9932700386006819, 1\n",
      "[1 1], 0.007222995644041263, 0\n",
      "RMS_err: 0.007205237244813308\n",
      "\n",
      "NN training succeded!\n"
     ]
    }
   ],
   "source": [
    "XOR = NeuralNetwork([2, 2, 1], activation='sigmoid', verbose=False)\n",
    "\n",
    "X = np.array([[0, 0],\n",
    "            [0, 1],\n",
    "            [1, 0],\n",
    "            [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "XOR.fit(X, \n",
    "         y,\n",
    "         steps=2*10**5,\n",
    "         learning_rate=2.5,\n",
    "         verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "Training Results(data, prediction, expected):\n",
      "[0.0000 0.0000], 0.7718880350987595, 0\n",
      "[0.0000 1.0000], 0.7735992396244773, 0\n",
      "[0.5000 1.0000], 0.7766090654027695, 0\n",
      "[0.0000 0.5000], 0.7727750900418506, 1\n",
      "[1.0000 0.0000], 0.7777931497836112, 1\n",
      "[1.0000 1.0000], 0.779215044218428, 1\n",
      "RMS_err: 0.5696697293798113\n",
      "\n",
      "Step: 20000\n",
      "Training Results(data, prediction, expected):\n",
      "[0.0000 0.0000], 0.7410023127135162, 0\n",
      "[0.0000 1.0000], 0.05245265250074869, 0\n",
      "[0.5000 1.0000], 0.036298100400263496, 0\n",
      "[0.0000 0.5000], 0.740792306690468, 1\n",
      "[1.0000 0.0000], 0.9996127138600228, 1\n",
      "[1.0000 1.0000], 0.9958185887244164, 1\n",
      "RMS_err: 0.32154821379161524\n",
      "\n",
      "Step: 40000\n",
      "Training Results(data, prediction, expected):\n",
      "[0.0000 0.0000], 0.029999259765185804, 0\n",
      "[0.0000 1.0000], 0.01929793926785371, 0\n",
      "[0.5000 1.0000], 0.02357227021083419, 0\n",
      "[0.0000 0.5000], 0.9682662015798937, 1\n",
      "[1.0000 0.0000], 0.9930568476442785, 1\n",
      "[1.0000 1.0000], 0.9841517048017284, 1\n",
      "RMS_err: 0.022856172604760788\n",
      "\n",
      "Step: 60000\n",
      "Training Results(data, prediction, expected):\n",
      "[0.0000 0.0000], 0.019940842989039635, 0\n",
      "[0.0000 1.0000], 0.012542431806690943, 0\n",
      "[0.5000 1.0000], 0.015137752751100736, 0\n",
      "[0.0000 0.5000], 0.9800796788238657, 1\n",
      "[1.0000 0.0000], 0.9950286037532752, 1\n",
      "[1.0000 1.0000], 0.9894937623566681, 1\n",
      "RMS_err: 0.014809998755980932\n",
      "\n",
      "Step: 80000\n",
      "Training Results(data, prediction, expected):\n",
      "[0.0000 0.0000], 0.015419932786158583, 0\n",
      "[0.0000 1.0000], 0.009924134053873553, 0\n",
      "[0.5000 1.0000], 0.011962088675593998, 0\n",
      "[0.0000 0.5000], 0.9838434552586354, 1\n",
      "[1.0000 0.0000], 0.9957859145087231, 1\n",
      "[1.0000 1.0000], 0.9915484396517136, 1\n",
      "RMS_err: 0.011758512208608069\n",
      "\n",
      "Step: 100000\n",
      "Training Results(data, prediction, expected):\n",
      "[0.0000 0.0000], 0.013205291054987573, 0\n",
      "[0.0000 1.0000], 0.008533243032921986, 0\n",
      "[0.5000 1.0000], 0.010248833730439572, 0\n",
      "[0.0000 0.5000], 0.9863755834280923, 1\n",
      "[1.0000 0.0000], 0.9963082440044816, 1\n",
      "[1.0000 1.0000], 0.9927835590783904, 1\n",
      "RMS_err: 0.010029667244151582\n",
      "\n",
      "Step: 120000\n",
      "Training Results(data, prediction, expected):\n",
      "[0.0000 0.0000], 0.011790642091801473, 0\n",
      "[0.0000 1.0000], 0.00752635682652855, 0\n",
      "[0.5000 1.0000], 0.009024358198995285, 0\n",
      "[0.0000 0.5000], 0.9879386076293442, 1\n",
      "[1.0000 0.0000], 0.9966493719822531, 1\n",
      "[1.0000 1.0000], 0.993579112758874, 1\n",
      "RMS_err: 0.008897902523595536\n",
      "\n",
      "NN training succeded!\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([2, 2, 2, 1], activation='sigmoid', verbose=False)\n",
    "\n",
    "X = np.array([[0, 0],\n",
    "            [0, 1],\n",
    "            [0.5, 1],\n",
    "            [0, 0.5],\n",
    "            [1, 0],\n",
    "            [1, 1]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "nn.fit(X, \n",
    "         y,\n",
    "         steps=2*10**5,\n",
    "         learning_rate=1.2,\n",
    "         verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
